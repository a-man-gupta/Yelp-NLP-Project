{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have spent a lot of time in New England. I have never seen a lobster roll shop outside of that area. So I went to Quincy's because I've seen it being built for a while. I didn't know what to expect. First of all it's very clean. Spotless.  The decor is more than I expected.  Pretty neat. Not cheesy and chain like. But definitely themed. The menu is simple. The prices are surprisingly very very reasonable. The rolls are loaded with meat.  I had the main liner. When it comes to lobster roll there is no special ingredient. The key is freshness and the amount of lobster for the price. Quincy's scored on both. The bisque is also very good. However if you want bisque with heaps of lobster you have to make sure you add it to your order. I ordered my bisque and it had no meat. And I asked why and they said lobster meat could be added. It was already pretty tasty but for a few dollars more the amount of fresh lobster added was incredible. This is a nice addition to the area and I will be back. I'm bringing my family next time. It's really laid out nice for families  and you can have a family lunch or dinner at a reasonable price which is a nice surprise.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv('data/test_raw_sample_reviews_PA.csv')\n",
    "\n",
    "reviews_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['text'] = reviews_df['text'].str.replace(r'http\\S+', '', regex=True)\n",
    "reviews_df['text'] = reviews_df['text'].str.replace(r'@\\w+', '', regex=True)\n",
    "reviews_df['text'] = reviews_df['text'].astype(str)\n",
    "reviews_df['text'] = reviews_df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_reviews_with_sep(review_texts, tokenizer):\n",
    "    \"\"\"\n",
    "    Combines all review texts into one string with the [SEP] token separating them.\n",
    "    \n",
    "    Parameters:\n",
    "    - review_texts (list of str): A list of review texts to be combined.\n",
    "    - tokenizer: The tokenizer used to obtain the [SEP] token.\n",
    "    \n",
    "    Returns:\n",
    "    - combined_text (str): The combined reviews separated by the [SEP] token.\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    # Get the separator token [SEP] using the tokenizer\n",
    "    sep_token = tokenizer.sep_token\n",
    "\n",
    "    print(f\"Separator token: {sep_token} + count: {count}\")\n",
    "    count += 1\n",
    "\n",
    "    # Combine all reviews with the [SEP] token in between\n",
    "    combined_text = f\" {sep_token} \".join(review_texts)\n",
    "\n",
    "    print(f\"Separator token: {sep_token}\")\n",
    "\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "\n",
    "#Try this model first to see if it is working current error is in the attention layer/embeddings\n",
    "class MultiAttentionTransformer(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", t5_model_name=\"t5-small\"):\n",
    "        super(MultiAttentionTransformer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "        # Add additional layers or mechanisms for multi-source attention (optional)\n",
    "        self.attn_layer = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "    def forward(self, reviews, business_metadata, prompt):\n",
    "        # Get the BERT embeddings for review history and business metadata\n",
    "        review_embeddings = self.bert_model(**reviews).last_hidden_state\n",
    "        business_embeddings = self.bert_model(**business_metadata).last_hidden_state\n",
    "\n",
    "        #Concatenate both review and business embeddings\n",
    "        combined_embeddings = torch.cat([review_embeddings, business_embeddings], dim=1)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.attn_layer(combined_embeddings, combined_embeddings, combined_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        generated_output = self.t5_model.generate(input_ids=prompt, encoder_outputs=attn_output)\n",
    "\n",
    "        # Decode the output to text\n",
    "        generated_review = self.t5_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "        return generated_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(reviews, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the reviews in the DataFrame using the provided tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "    - reviews_df (DataFrame): The DataFrame containing the reviews.\n",
    "    - tokenizer: The tokenizer used for tokenization.\n",
    "    \n",
    "    Returns:\n",
    "    - tokenized_reviews (list of list of int): A list of tokenized reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that all elements in reviews are strings\n",
    "\n",
    "    # Tokenize each review and convert to IDs\n",
    "    encoded_reviews = tokenizer(reviews, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    # Move the model and data to the GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenized_reviews = {key: value.to(device) for key, value in encoded_reviews.items()}\n",
    "\n",
    "\n",
    "\n",
    "    return tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionTransformerWithCopy(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", t5_model_name=\"t5-small\"):\n",
    "        super(MultiAttentionTransformerWithCopy, self).__init__()\n",
    "\n",
    "        # Initialize BERT tokenizer and model\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        # Initialize T5 tokenizer and model\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "        # Add additional layers or mechanisms for multi-source attention (optional)\n",
    "        self.attn_layer = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "    def forward(self, review_texts, business_metadata, prompt, past_reviews):\n",
    "        \"\"\"\n",
    "        Forward pass for the model that includes a copy mechanism and processes a list of reviews.\n",
    "        \n",
    "        Parameters:\n",
    "        - review_texts: List of text strings for reviews.\n",
    "        - business_metadata: Metadata about the business (could include info like location, type, etc.).\n",
    "        - prompt: The user's input or prompt for the generated review.\n",
    "        - past_reviews: Texts of past reviews for the same business.\n",
    "        \n",
    "        Returns:\n",
    "        - Generated review text.\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize and encode all reviews in the review_texts list\n",
    "        review_tokens = self.bert_tokenizer(review_texts, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False)\n",
    "        business_tokens = self.bert_tokenizer(business_metadata, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False)\n",
    "        past_reviews_tokens = self.bert_tokenizer(past_reviews, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False)\n",
    "\n",
    "        # Get the BERT embeddings for review history, business metadata, and past reviews\n",
    "        review_embeddings = self.bert_model(**review_tokens).last_hidden_state\n",
    "        business_embeddings = self.bert_model(**business_tokens).last_hidden_state\n",
    "        past_reviews_embeddings = self.bert_model(**past_reviews_tokens).last_hidden_state\n",
    "\n",
    "        # Concatenate all embeddings: review + business metadata + past reviews\n",
    "        combined_embeddings = torch.cat([review_embeddings, business_embeddings, past_reviews_embeddings], dim=1)\n",
    "\n",
    "        # Apply multi-source attention (simple multihead attention for demonstration)\n",
    "        attn_output, _ = self.attn_layer(combined_embeddings, combined_embeddings, combined_embeddings)\n",
    "\n",
    "        # Implement the copy mechanism: We extract attention scores to decide which tokens to copy\n",
    "        copy_tokens = self.select_copy_tokens(attn_output, past_reviews_embeddings)\n",
    "\n",
    "        # Prepare the prompt for the T5 model\n",
    "        prompt_input = self.t5_tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Use the attention output as the decoder input and generate the review with T5\n",
    "        decoder_input_ids = prompt_input['input_ids']\n",
    "        generated_output = self.t5_model.generate(input_ids=decoder_input_ids, decoder_input_ids=attn_output)\n",
    "\n",
    "        # Decode the output to text\n",
    "        generated_review = self.t5_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Integrate copy mechanism into the generated review\n",
    "        generated_review_with_copy = self.apply_copy_mechanism(generated_review, copy_tokens)\n",
    "\n",
    "        return generated_review_with_copy\n",
    "\n",
    "    def select_copy_tokens(self, attn_output, past_reviews_embeddings):\n",
    "        \"\"\"\n",
    "        Select which tokens from past reviews to copy based on attention scores.\n",
    "        \n",
    "        Parameters:\n",
    "        - attn_output: The output from the multihead attention layer.\n",
    "        - past_reviews_embeddings: Embeddings from past reviews.\n",
    "        \n",
    "        Returns:\n",
    "        - copy_tokens: The tokens that will be copied from past reviews.\n",
    "        \"\"\"\n",
    "        # Calculate the attention scores\n",
    "        attention_scores = torch.mean(attn_output, dim=1)  # Averaging across attention heads for simplicity\n",
    "        \n",
    "        # Ensure that the top-k tokens are within bounds\n",
    "        k = 5  # Number of top tokens to select\n",
    "        num_tokens = attention_scores.size(1)  # Get the number of tokens in the attention output\n",
    "        num_past_tokens = past_reviews_embeddings.size(1)  # Get the number of tokens in past reviews\n",
    "        if num_tokens > num_past_tokens:\n",
    "            # Ensure k does not exceed the number of available tokens\n",
    "            k = min(k, num_past_tokens)\n",
    "        else:\n",
    "            # If there are fewer tokens in past reviews, adjust k accordingly\n",
    "            k = min(k, num_tokens)\n",
    "        \n",
    "        # Get the top k attention scores\n",
    "        top_attention_indices = torch.topk(attention_scores, k=k, dim=-1).indices  # Select top k tokens\n",
    "\n",
    "        # Extract the tokens corresponding to the top attention indices\n",
    "        copy_tokens = past_reviews_embeddings[:, top_attention_indices]\n",
    "\n",
    "        return copy_tokens\n",
    "\n",
    "    def apply_copy_mechanism(self, generated_review, copy_tokens):\n",
    "        \"\"\"\n",
    "        Apply the copy mechanism to the generated review by inserting copied tokens.\n",
    "        \n",
    "        Parameters:\n",
    "        - generated_review: The review generated by T5.\n",
    "        - copy_tokens: The tokens selected for copying from past reviews.\n",
    "        \n",
    "        Returns:\n",
    "        - modified_review: The final review with copied phrases inserted.\n",
    "        \"\"\"\n",
    "        # In this simple case, we'll append the copied tokens to the generated review.\n",
    "        # More sophisticated copy mechanisms would replace or interleave tokens in the review.\n",
    "\n",
    "        copy_text = \" \".join([self.bert_tokenizer.decode(token) for token in copy_tokens[0]])  # Decode copied tokens\n",
    "        modified_review = generated_review + \" \" + copy_text\n",
    "\n",
    "        return modified_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all reviews for a given business ID\n",
    "def get_reviews_for_business(business_id, reviews_df):\n",
    "    # Filter the dataset for the given business_id\n",
    "    business_reviews = reviews_df[reviews_df['business_id'] == business_id]\n",
    "    return business_reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_for_user(user_id, reviews_df):\n",
    "    # Filter the dataset for the given user_id\n",
    "    user_reviews = reviews_df[reviews_df['user_id'] == user_id]\n",
    "    return user_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def generate_review_with_t5(reviews, business_metadata, prompt, tokenizer, model, max_len=512):\n",
    "    \"\"\"\n",
    "    Generates a review based on all reviews, business metadata, and a given prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    - reviews (list of str): List of review texts.\n",
    "    - business_metadata (str): The business metadata (e.g., business description).\n",
    "    - prompt (str): The input prompt to guide the generation.\n",
    "    - tokenizer: Pre-trained T5 tokenizer.\n",
    "    - model: Pre-trained T5 model.\n",
    "    - max_len (int): Maximum length for padding/truncation.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_review (str): The generated review text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize and encode the reviews and business metadata\n",
    "    review_input = tokenizer(reviews, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "    business_input = tokenizer(business_metadata, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "\n",
    "    # Combine reviews and business metadata into one input string\n",
    "    combined_input_ids = torch.cat([review_input['input_ids'], business_input['input_ids']], dim=1)\n",
    "\n",
    "    # Concatenate with the prompt\n",
    "    prompt_input = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=max_len)\n",
    "\n",
    "    # Get the encoder's output (reviews + business metadata)\n",
    "    encoder_input_ids = combined_input_ids.to(model.device)\n",
    "    attention_mask = torch.cat([review_input['attention_mask'], business_input['attention_mask']], dim=1).to(model.device)\n",
    "\n",
    "    # Generate the output based on the input + prompt\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=prompt_input['input_ids'], \n",
    "        attention_mask=attention_mask, \n",
    "        decoder_input_ids=encoder_input_ids, \n",
    "        max_length=max_len\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_review = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separator token: None + count: 0\n",
      "Separator token: None\n",
      "Separator token: None + count: 0\n",
      "Separator token: None\n"
     ]
    }
   ],
   "source": [
    "sample_reviews = reviews_df.sample(frac=0.001, random_state=42)  # Sample 10% of the reviews for demonstration\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#input_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "business_id = 'IkY2ticzHEn4QFn8hQLSWg'  # Example business ID \n",
    "business_metadata = get_reviews_for_business(business_id, reviews_df)\n",
    "business_metadata = combine_reviews_with_sep(business_metadata['text'], tokenizer)\n",
    "#business_metadata = tokenize_reviews(business_metadata, tokenizer)\n",
    "user_id = '_BcWyKQL16ndpBdggh2kNA'  # Example user ID\n",
    "user_reviews = get_reviews_for_user(user_id, reviews_df)\n",
    "#user_reviews = combine_reviews_with_sep(user_reviews['text'], tokenizer)\n",
    "#user_reviews = tokenize_reviews(user_reviews, tokenizer)\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "review_text = combine_reviews_with_sep(sample_reviews['text'], tokenizer)\n",
    "#review_text = tokenize_reviews(review_text, tokenizer)\n",
    "\n",
    "prompt = \"Generate a positive review for Geno's SteakHouse that mentions the food quality and service.\"\n",
    "#prompt = input_tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input length of decoder_input_ids is 599, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate a review\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generated_review \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_review_with_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Review: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_review\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mgenerate_review_with_t5\u001b[0;34m(reviews, business_metadata, prompt, tokenizer, model, max_len)\u001b[0m\n\u001b[1;32m     33\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([review_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], business_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Generate the output based on the input + prompt\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Decode the generated text\u001b[39;00m\n\u001b[1;32m     44\u001b[0m generated_review \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/transformers/generation/utils.py:2321\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2319\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[1;32m   2327\u001b[0m max_cache_length \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/site-packages/transformers/generation/utils.py:1554\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1553\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m     )\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1564\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of decoder_input_ids is 599, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "# Generate a review\n",
    "generated_review = generate_review_with_t5(review_text, business_metadata, prompt, tokenizer, model)\n",
    "\n",
    "print(f\"Generated Review: {generated_review}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
